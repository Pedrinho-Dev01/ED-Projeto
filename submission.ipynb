{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Data Mining to understand fake news and misinformation in election cycles\n",
    "\n",
    "## Authors\n",
    "\n",
    "Salom√© Dias     - 118163\n",
    "Daniel Pedrinho - 107378\n",
    "\n",
    "## Project Objective\n",
    "\n",
    "The objective of this project is to understand the spread of fake news and misinformation in election cycles, using data mining techniques to analyze the data.\n",
    "\n",
    "## Similar Works\n",
    "\n",
    "1. https://ieeexplore.ieee.org/document/10068440\n",
    "A. Matheven and B. V. D. Kumar, \"Fake News Detection Using Deep Learning and Natural Language Processing,\" 2022 9th International Conference on Soft Computing & Machine Intelligence (ISCMI), Toronto, ON, Canada, 2022, pp. 11-14, doi: 10.1109/ISCMI56532.2022.10068440. keywords: {Industries;Deep learning;Training;Social networking (online);Natural language processing;Fake news;Machine intelligence;fake news;deep Learning;natural language processing}\n",
    "\n",
    "2. https://ieeexplore.ieee.org/document/9641517\n",
    "X. Jose, S. D. M. Kumar and P. Chandran, \"Characterization, Classification and Detection of Fake News in Online Social Media Networks,\" 2021 IEEE Mysore Sub Section International Conference (MysuruCon), Hassan, India, 2021, pp. 759-765, doi: 10.1109/MysuruCon52639.2021.9641517. keywords: {Costs;Social networking (online);IEEE Sections;Conferences;Pressing;Bidirectional control;Fake news;Online fake news;Social media analytics;Natural Language Processing;Machine learning;Fake news detection}\n",
    "\n",
    "3. https://ieeexplore.ieee.org/document/9702824\n",
    "F. W. Wibowo, A. Dahlan and Wihayati, \"Detection of Fake News and Hoaxes on Information from Web Scraping using Classifier Methods,\" 2021 4th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI), Yogyakarta, Indonesia, 2021, pp. 178-183, doi: 10.1109/ISRITI54043.2021.9702824. keywords: {Support vector machines;Stochastic processes;Artificial neural networks;Boosting;Natural language processing;Data models;Decision trees;classification;fake news;hoax;Indonesian language;nlp;web scraping}\n",
    "\n",
    "## Data Aquisition\n",
    "\n",
    "The query currently is defined in 2 parts:\n",
    "\n",
    "1. Part 1 relates to the \"base\" data, that will be used as a baseline for comparison.\n",
    "2. Part 2 relates to the \"target\" data, that is, the data we are interested in.\n",
    "\n",
    "Both queries are built in the same way, with the only difference being the time period of the search\n",
    "To filtrate the data, several news sources are selected, based of this list: https://today.yougov.com/ratings/entertainment/popularity/news-websites/all\n",
    "\n",
    "From this list were excluded same organization with different domains and sports websites.\n",
    "\n",
    "The query is done once for each site, and the results are stored in an external file, keeping the original arquivo.pt data structure.\n",
    "\n",
    "The data is acquired from the arquivo.pt TextSearch API seems lacking in quantity, and the website search further affirms this, as the results amount stated is far greater than what is presented both by the API requests, and the website search.\n",
    "\n",
    "Further more, some of the news sources selected return no results, which we believe to be odd, since both the period of the search and its terms are very broad and well documented.\n",
    "\n",
    "As such, we are considering changing the API from the Arquivo.pt API (Full-text & URL search) to the CDX-server API (URL search) or Memento API (URL search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "default = 'https://arquivo.pt/textsearch?q='\n",
    "election_time = '&from=20200203000000&to=20201103000000'\n",
    "non_election_time = '&from=20180203000000&to=20181103000000'\n",
    "pretty_print = \"&prettyPrint=true\"\n",
    "\n",
    "news_sites = ['&siteSearch=www.cbs.com', \n",
    "              '&siteSearch=www.nbc.com', \n",
    "              '&siteSearch=www.washingtonpost.com', \n",
    "              '&siteSearch=www.bbc.com',\n",
    "              '&siteSearch=www.forbes.com',\n",
    "              '&siteSearch=www.nytimes.com', \n",
    "              '&siteSearch=www.foxnews.com', \n",
    "              '&siteSearch=www.cnn.com']\n",
    "\n",
    "def request_api(query, is_election, site):\n",
    "\n",
    "    if query:\n",
    "\n",
    "        if is_election == 1:\n",
    "            response = requests.get(default + query + election_time + site +pretty_print)\n",
    "            return response\n",
    "        elif is_election == 0:\n",
    "            response = requests.get(default + query + non_election_time + site + pretty_print)\n",
    "            return response\n",
    "    else:\n",
    "        print(\"No query provided\")\n",
    "        return\n",
    "    \n",
    "input_query = input(\"Enter a query: \")\n",
    "is_election = int(input(\"Is it election time? (1 for yes, 0 for no): \"))\n",
    "\n",
    "# if output.txt exists, delete it\n",
    "if os.path.exists('output_election.txt'):\n",
    "    os.remove('output_election.txt')\n",
    "\n",
    "if os.path.exists('output_non_election.txt'):\n",
    "    os.remove('output_non_election.txt')\n",
    "\n",
    "for site in news_sites:\n",
    "    response = request_api(input_query, is_election, site)\n",
    "    # dump the response to a file without overwriting\n",
    "    if is_election == 1:\n",
    "        with open('output_election.txt', 'a') as f:\n",
    "            f.write('\\n############################ ' + site + ' ############################\\n')\n",
    "            f.write(response.text)\n",
    "            f.write('##############################\\n')\n",
    "    else:\n",
    "        with open('output_non_election.txt', 'a') as f:\n",
    "            f.write('\\n############################ ' + site + ' ############################\\n')\n",
    "            f.write(response.text)\n",
    "            f.write('##############################\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
